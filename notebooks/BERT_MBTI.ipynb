{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75c19e1",
   "metadata": {},
   "source": [
    "# 1. Data Loading, Cleaning & Splitting (MBTI Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394ccae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# src/data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======================\n",
    "# 1. Load & inspect data\n",
    "# ======================\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path, on_bad_lines=\"skip\", engine=\"python\")\n",
    "    print(\"Sá»‘ máº«u:\", len(df))\n",
    "    print(\"Sá»‘ loáº¡i MBTI khÃ¡c nhau:\", df[\"type\"].nunique())\n",
    "    print(df[\"type\"].value_counts())\n",
    "    return df\n",
    "\n",
    "# ======================\n",
    "# 2. Encode MBTI -> 4 nhÃ£n binary\n",
    "# ======================\n",
    "def mbti_to_binary(mbti):\n",
    "    return {\n",
    "        \"IE\": 0 if mbti[0] == \"I\" else 1,\n",
    "        \"NS\": 0 if mbti[1] == \"N\" else 1,\n",
    "        \"TF\": 0 if mbti[2] == \"T\" else 1,\n",
    "        \"JP\": 0 if mbti[3] == \"J\" else 1,\n",
    "    }\n",
    "\n",
    "def add_binary_columns(df):\n",
    "    df = df.copy()\n",
    "    df[\"mbti_IE\"] = df[\"type\"].apply(lambda x: mbti_to_binary(x)[\"IE\"])\n",
    "    df[\"mbti_NS\"] = df[\"type\"].apply(lambda x: mbti_to_binary(x)[\"NS\"])\n",
    "    df[\"mbti_TF\"] = df[\"type\"].apply(lambda x: mbti_to_binary(x)[\"TF\"])\n",
    "    df[\"mbti_JP\"] = df[\"type\"].apply(lambda x: mbti_to_binary(x)[\"JP\"])\n",
    "    return df\n",
    "\n",
    "# ======================\n",
    "# 3. LÃ m sáº¡ch vÄƒn báº£n\n",
    "# ======================\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)             # bá» link\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?']\", \" \", text)          # bá» kÃ½ tá»± láº¡\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()                  # gom khoáº£ng tráº¯ng\n",
    "    return text\n",
    "\n",
    "# ======================\n",
    "# 4. Dataset class cho BERT\n",
    "# ======================\n",
    "class MBTIDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=256, augment_fn=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.augment_fn = augment_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = clean_text(row.get(\"posts\", \"\"))  # ðŸ‘ˆ giá»¯ nguyÃªn toÃ n bá»™ posts cá»§a user\n",
    "\n",
    "        if self.augment_fn:\n",
    "            try:\n",
    "                text = self.augment_fn(text)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        labels = torch.tensor(\n",
    "            [row[\"mbti_IE\"], row[\"mbti_NS\"], row[\"mbti_TF\"], row[\"mbti_JP\"]],\n",
    "            dtype=torch.float\n",
    "        )\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# 5. Cháº¡y trá»±c tiáº¿p Ä‘á»ƒ tiá»n xá»­ lÃ½ & lÆ°u\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data(\"/kaggle/input/mbti-type/mbti_1.csv\")\n",
    "    df = add_binary_columns(df)\n",
    "    df[\"posts\"] = df[\"posts\"].apply(clean_text)   # lÃ m sáº¡ch toÃ n bá»™ posts (khÃ´ng explode)\n",
    "    df.to_csv(\"/kaggle/working/mbti_clean.csv\", index=False)\n",
    "    print(\"âœ… ÄÃ£ lÆ°u xong mbti_clean.csv (giá»¯ nguyÃªn 1 dÃ²ng/user, cÃ³ 4 nhÃ£n binary)\")\n",
    "    \n",
    "    \n",
    "    label_cols = [\"mbti_IE\", \"mbti_NS\", \"mbti_TF\", \"mbti_JP\"]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(label_cols):\n",
    "        counts = df[col].value_counts().sort_index()  # 0 vÃ  1\n",
    "        counts.plot(kind=\"bar\", ax=axes[i])\n",
    "        axes[i].set_title(f\"PhÃ¢n phá»‘i nhÃ£n {col}\")\n",
    "        axes[i].set_xticklabels([\"0\", \"1\"], rotation=0)\n",
    "        for idx, val in enumerate(counts):\n",
    "            axes[i].text(idx, val, str(val), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca11518",
   "metadata": {},
   "source": [
    "# 2. MBTIModel: BERT-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd61b65e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class MBTIModel(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", dropout=0.4,\n",
    "                 use_hidden_layer=True, pooling=\"cls+mean\"):\n",
    "        super(MBTIModel, self).__init__()\n",
    "\n",
    "        # Load full BERT (khÃ´ng freeze)\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        self.pooling = pooling\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        input_dim = hidden_size * 2 if pooling == \"cls+mean\" else hidden_size\n",
    "\n",
    "        if use_hidden_layer:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim // 2),\n",
    "                nn.BatchNorm1d(input_dim // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(input_dim // 2, 4)\n",
    "            )\n",
    "        else:\n",
    "            self.classifier = nn.Linear(input_dim, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        if self.pooling == \"cls\":\n",
    "            pooled = outputs.last_hidden_state[:, 0, :]\n",
    "        elif self.pooling == \"mean\":\n",
    "            pooled = (outputs.last_hidden_state * attention_mask.unsqueeze(-1)).sum(1)\n",
    "            pooled = pooled / attention_mask.sum(1, keepdim=True)\n",
    "        elif self.pooling == \"max\":\n",
    "            masked = outputs.last_hidden_state.masked_fill(\n",
    "                attention_mask.unsqueeze(-1) == 0, -1e9\n",
    "            )\n",
    "            pooled = masked.max(1).values\n",
    "        elif self.pooling == \"cls+mean\":\n",
    "            cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "            mean_emb = (outputs.last_hidden_state * attention_mask.unsqueeze(-1)).sum(1)\n",
    "            mean_emb = mean_emb / attention_mask.sum(1, keepdim=True)\n",
    "            pooled = torch.cat([cls_emb, mean_emb], dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling: {self.pooling}\")\n",
    "\n",
    "        x = self.dropout(pooled)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac137e5",
   "metadata": {},
   "source": [
    "3. MBTI Model Training (BERT + BCEWithLogitsLoss + Mixed Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58653b54",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random \n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # deterministic cuDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42) \n",
    "# ======================\n",
    "# Train + Eval helpers\n",
    "# ======================\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device, loss_fn, epoch, epochs, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def eval_epoch(model, dataloader, device, loss_fn, epoch, epochs, mode=\"Valid\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs} [{mode}]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Láº¥y xÃ¡c suáº¥t + dá»± Ä‘oÃ¡n nhá»‹ phÃ¢n\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "            # LÆ°u labels dÆ°á»›i dáº¡ng numpy\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds)\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_preds = np.vstack(all_preds)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return total_loss / len(dataloader), acc, f1\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Config\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    batch_size = 8\n",
    "    max_len = 256\n",
    "    lr = 2e-5\n",
    "    epochs = 50\n",
    "    num_workers = 4\n",
    "    save_dir = \"/kaggle/working/\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    best_model_path = os.path.join(save_dir, \"mbti_best.pt\")\n",
    "    ckpt_path = os.path.join(save_dir, \"mbti_ckpt.pt\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # ======================\n",
    "    # Load & preprocess data\n",
    "    # ======================\n",
    "    data_dir = \"/kaggle/working/\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    clean_csv = os.path.join(data_dir, \"mbti_clean.csv\")\n",
    "    if not os.path.exists(clean_csv):\n",
    "        df = load_data(os.path.join(data_dir, \"mbti_1.csv\"))\n",
    "        df = add_binary_columns(df)\n",
    "        df[\"posts\"] = df[\"posts\"].apply(clean_text)\n",
    "        df.to_csv(clean_csv, index=False)\n",
    "\n",
    "    df = pd.read_csv(clean_csv)\n",
    "\n",
    "    # Split train/valid/test (70/20/10)\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, test_size=0.3, random_state=42, stratify=df[\"type\"]\n",
    "    )\n",
    "    valid_df, test_df = train_test_split(\n",
    "        temp_df, test_size=0.3333, random_state=42, stratify=temp_df[\"type\"]\n",
    "    )\n",
    "    test_df.to_csv(os.path.join(data_dir, \"test.csv\"), index=False)\n",
    "\n",
    "    print(f\"Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "    # Tokenizer + Dataset + DataLoader\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    train_dataset = MBTIDataset(train_df, tokenizer, max_len=max_len)\n",
    "    valid_dataset = MBTIDataset(valid_df, tokenizer, max_len=max_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # Model + Optimizer + Loss\n",
    "    model = MBTIModel(model_name=model_name, pooling=\"cls+mean\", dropout=0.4).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    # pos_weight cho BCE\n",
    "    labels = train_df[[\"mbti_IE\",\"mbti_NS\",\"mbti_TF\",\"mbti_JP\"]].values\n",
    "    pos_weights = (labels.shape[0] - labels.sum(axis=0)) / labels.sum(axis=0)\n",
    "    pos_weights = torch.tensor(pos_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Resume checkpoint náº¿u cÃ³\n",
    "    start_epoch, best_valid_f1 = 0, 0.0\n",
    "    if os.path.exists(ckpt_path):\n",
    "        print(f\"ðŸ”„ Found checkpoint: {ckpt_path}, loading...\")\n",
    "        ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(ckpt[\"model_state\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
    "        start_epoch = ckpt[\"epoch\"] + 1\n",
    "        best_valid_f1 = ckpt[\"best_valid_f1\"]\n",
    "        print(f\"ðŸ‘‰ Resume training from epoch {start_epoch}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, device, loss_fn, epoch, epochs, scaler)\n",
    "        valid_loss, valid_acc, valid_f1 = eval_epoch(model, valid_loader, device, loss_fn, epoch, epochs)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Valid Loss: {valid_loss:.4f} | \"\n",
    "              f\"Acc: {valid_acc:.4f} | Macro-F1: {valid_f1:.4f}\")\n",
    "\n",
    "        # Save best model (Macro-F1)\n",
    "        if valid_f1 > best_valid_f1:\n",
    "            best_valid_f1 = valid_f1\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"âœ… Saved best model to {best_model_path}\")\n",
    "\n",
    "        # LuÃ´n lÆ°u checkpoint má»—i epoch\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"scheduler_state\": scheduler.state_dict(),\n",
    "            \"best_valid_f1\": best_valid_f1,\n",
    "        }, ckpt_path)\n",
    "        print(f\"ðŸ’¾ Saved checkpoint (epoch {epoch + 1}) to {ckpt_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddca910",
   "metadata": {},
   "source": [
    "# 4. ðŸ§ª MBTI Model Evaluation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2dc9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Evaluation\n",
    "# ======================\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# ----------------------\n",
    "# Core evaluation\n",
    "# ----------------------\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels, all_preds, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probs)\n",
    "\n",
    "    return (\n",
    "        np.vstack(all_labels),\n",
    "        np.vstack(all_preds),\n",
    "        np.vstack(all_probs)\n",
    "    )\n",
    "\n",
    "# ----------------------\n",
    "# Confusion matrices\n",
    "# ----------------------\n",
    "def plot_confusion_matrices(y_true, y_pred, axes, save_dir=None):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n",
    "        ax.set_title(f\"Confusion Matrix - {axes[i]}\")\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"True\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        path = os.path.join(save_dir, \"confusion_matrices.png\")\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "        print(f\"âœ… Saved confusion matrix to {path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# Probability distribution\n",
    "# ----------------------\n",
    "def plot_probability_distribution(probs, axes, save_dir=None):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        sns.histplot(probs[:, i], bins=20, kde=True, ax=ax)\n",
    "        ax.set_title(f\"Predicted Prob Distribution - {axes[i]}\")\n",
    "        ax.set_xlabel(\"Probability\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        path = os.path.join(save_dir, \"prob_dist.png\")\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "        print(f\"âœ… Saved probability distribution to {path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# Error analysis\n",
    "# ----------------------\n",
    "def error_analysis(test_df, y_true, y_pred, y_probs, axes, n_samples=15):\n",
    "    errors = []\n",
    "    for i in range(len(y_true)):\n",
    "        for j, axis in enumerate(axes):\n",
    "            if y_true[i, j] != y_pred[i, j]:\n",
    "                errors.append({\n",
    "                    \"index\": i,\n",
    "                    \"axis\": axis,\n",
    "                    \"true\": int(y_true[i, j]),\n",
    "                    \"pred\": int(y_pred[i, j]),\n",
    "                    \"prob\": float(y_probs[i, j]),\n",
    "                    \"text\": str(test_df.iloc[i][\"posts\"])[:200] + \"...\"\n",
    "                })\n",
    "\n",
    "    errors_df = pd.DataFrame(errors)\n",
    "    os.makedirs(\"reports\", exist_ok=True)\n",
    "    errors_df.to_csv(\"reports/error_samples.csv\", index=False)\n",
    "    print(f\"\\nâŒ Saved {len(errors_df)} misclassified samples to reports/error_samples.csv\")\n",
    "\n",
    "    if len(errors_df) > 0:\n",
    "        print(\"\\n=== Sample Errors ===\")\n",
    "        print(errors_df.sample(min(n_samples, len(errors_df))))\n",
    "\n",
    "# ----------------------\n",
    "# Main\n",
    "# ----------------------\n",
    "def main():\n",
    "\n",
    "    # Config\n",
    "    set_seed(42)\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    batch_size = 8\n",
    "    max_len = 256\n",
    "    model_path = \"/kaggle/working/mbti_best.pt\"\n",
    "    test_csv = \"/kaggle/working/test.csv\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Load test set\n",
    "    test_df = pd.read_csv(test_csv)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    test_dataset = MBTIDataset(test_df, tokenizer, max_len=max_len)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Load model\n",
    "    model = MBTIModel(model_name=model_name, pooling=\"cls+mean\", dropout=0.4)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    # Evaluate\n",
    "    y_true, y_pred, y_probs = evaluate(model, test_loader, device)\n",
    "\n",
    "    axes = [\"IE\", \"NS\", \"TF\", \"JP\"]\n",
    "    metrics = {}\n",
    "\n",
    "    print(\"\\n=== Metrics per axis ===\")\n",
    "    for i, axis in enumerate(axes):\n",
    "        acc = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "        f1 = f1_score(y_true[:, i], y_pred[:, i], average=\"macro\")\n",
    "        metrics[axis] = {\"Accuracy\": acc, \"Macro-F1\": f1}\n",
    "        print(f\"{axis}: Acc={acc:.4f}, Macro-F1={f1:.4f}\")\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics).T\n",
    "    os.makedirs(\"reports\", exist_ok=True)\n",
    "    metrics_df.to_csv(\"reports/metrics.csv\")\n",
    "    print(\"\\nâœ… Metrics saved to reports/metrics.csv\")\n",
    "\n",
    "    # Save classification report (multi-label)\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, output_dict=True, zero_division=0\n",
    "    )\n",
    "    pd.DataFrame(report).to_csv(\"reports/classification_report.csv\")\n",
    "    print(\"âœ… Classification report saved to reports/classification_report.csv\")\n",
    "\n",
    "    # Visualization\n",
    "    plot_confusion_matrices(y_true, y_pred, axes, save_dir=\"reports/figures\")\n",
    "    plot_probability_distribution(y_probs, axes, save_dir=\"reports/figures\")\n",
    "\n",
    "    # Error analysis\n",
    "    error_analysis(test_df, y_true, y_pred, y_probs, axes)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"reports\", exist_ok=True)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65756c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
